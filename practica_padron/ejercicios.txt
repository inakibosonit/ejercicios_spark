1- Creación de tablas en formato texto.

CREATE DATABASE datos_padron;

Crear la tabla de datos padron_txt con todos los campos del fichero CSV y cargar los
datos mediante el comando LOAD DATA LOCAL INPATH. La tabla tendrá formato
texto y tendrá como delimitador de campo el caracter ';' y los campos que en el
documento original están encerrados en comillas dobles '"' no deben estar
envueltos en estos caracteres en la tabla de Hive (es importante indicar esto
utilizando el serde de OpenCSV, si no la importación de las variables que hemos
indicado como numéricas fracasará ya que al estar envueltos en comillas los toma
como strings) y se deberá omitir la cabecera del fichero de datos al crear la tabla.

CREATE TABLE padron_txt (COD_DISTRITO int, DESC_DISTRITO string, COD_DIST_BARRIO int, DESC_BARRIO string, COD_BARRIO int, COD_DIST_SECCION int, COD_SECCION int, COD_EDAD_INT int, EspanolesHombres int, EspanolesMujeres int, ExtranjerosHombres int, ExtranjerosMujeres int) 
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
WITH SERDEPROPERTIES 
(
    "separatorChar" = '\u0059',
    "quoteChar"     = "\""
)   STORED AS TEXTFILE;


LOAD DATA LOCAL INPATH './Rango_Edades_Seccion_202112.csv' OVERWRITE INTO TABLE padron_txt;

SELECT COUNT(*) FROM padron_txt;

Hacer trim sobre los datos para eliminar los espacios innecesarios guardando la
tabla resultado como padron_txt_2. (Este apartado se puede hacer creando la tabla
con una sentencia CTAS.)

CREATE TABLE padron_txt_2 AS Select COD_DISTRITO, trim(DESC_DISTRITO) as DESC_DISTRITO, COD_DIST_BARRIO, trim(DESC_BARRIO) as DESC_BARRIO, COD_BARRIO, COD_DIST_SECCION, COD_SECCION,COD_EDAD_INT,EspanolesHombres, EspanolesMujeres, ExtranjerosHombres, ExtranjerosMujeres from padron_txt;

-- Investigar y entender la diferencia de incluir la palabra LOCAL en el comando LOAD DATA.

-- load data sin el local realiza una insercion de los datos desde la propia maquina y fichero, por otro lado incluyendo local la consola de hive buscara e incluira los datos desde HDFS.

En este momento te habrás dado cuenta de un aspecto importante, los datos nulos
de nuestras tablas vienen representados por un espacio vacío y no por un
identificador de nulos comprensible para la tabla. Esto puede ser un problema para
el tratamiento posterior de los datos. Podrías solucionar esto creando una nueva
tabla utiliando sentencias case when que sustituyan espacios en blanco por 0. Para
esto primero comprobaremos que solo hay espacios en blanco en las variables
numéricas correspondientes a las últimas 4 variables de nuestra tabla (podemos
hacerlo con alguna sentencia de HiveQL) y luego aplicaremos las sentencias case
when para sustituir por 0 los espacios en blanco. (Pista: es útil darse cuenta de que
un espacio vacío es un campo con longitud 0). Haz esto solo para la tabla
padron_txt.

INSERT OVERWRITE TABLE padron_txt 
SELECT COD_DISTRITO, DESC_DISTRITO, COD_DIST_BARRIO, DESC_BARRIO, COD_BARRIO, COD_DIST_SECCION, COD_SECCION, COD_EDAD_INT, 
CASE WHEN length(EspanolesHombres) > 0 THEN EspanolesHombres ELSE '0' END AS EspanolesHombres,
CASE WHEN length(EspanolesMujeres) > 0 THEN EspanolesMujeres ELSE '0' END AS EspanolesMujeres,
CASE WHEN length(ExtranjerosHombres) > 0 THEN ExtranjerosHombres ELSE '0' END AS ExtranjerosHombres,
CASE WHEN length(ExtranjerosMujeres) > 0 THEN ExtranjerosMujeres ELSE '0' END AS ExtranjerosMujeres
FROM padron_txt);



Una manera tremendamente potente de solucionar todos los problemas previos
(tanto las comillas como los campos vacíos que no son catalogados como null y los
espacios innecesarios) es utilizar expresiones regulares (regex) que nos proporciona
OpenCSV.
Para ello utilizamos :
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.RegexSerDe'
WITH SERDEPROPERTIES ('input.regex'='XXXXXXX')
Donde XXXXXX representa una expresión regular que debes completar y que
identifique el formato exacto con el que debemos interpretar cada una de las filas de
nuestro CSV de entrada. Para ello puede ser útil el portal "regex101". Utiliza este método
para crear de nuevo la tabla padron_txt_2.


create table padron_txt_2 (*) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.RegexSerDe'
WITH SERDEPROPERTIES ('input.regex'='^[ '"]+|[ '"]+$|( ){2,}') stored as textfile


/* ^[ '"]+ matches any space or single or double quote one or more from start of string and similarly [ '"]+$ also matches any 
space or single or doublequote one or more at the end of string and gets replaced with empty string where as when ( ){2,} matches, 
which remains to be matched in the middle of string, gets replaced with a single space as captured in group1. */


-- ¿Qué es CTAS?

Creación de una tabla a partir de los resultados de una consulta (CTAS). 
Una consulta CREATE TABLE AS SELECT (CTAS) crea una nueva tabla a partir de los resultados de una instrucción SELECT de otra consulta. 

Crear tabla Hive padron_parquet (cuyos datos serán almacenados en el formato
columnar parquet) a través de la tabla padron_txt mediante un CTAS.

CREATE TABLE padron_parquet STORED AS PARQUET AS SELECT * FROM padron_txt;

Crear tabla Hive padron_parquet_2 a través de la tabla padron_txt_2 mediante un
CTAS. En este punto deberíamos tener 4 tablas, 2 en txt (padron_txt y
padron_txt_2, la primera con espacios innecesarios y la segunda sin espacios
innecesarios) y otras dos tablas en formato parquet (padron_parquet y
padron_parquet_2, la primera con espacios y la segunda sin ellos).

-- OPCION 1

CREATE TABLE padron_parquet_2 STORED AS PARQUET AS SELECT * FROM padron_txt_2;


-- OPCION 2

Opcionalmente también se pueden crear las tablas directamente desde 0 (en lugar
de mediante CTAS) en formato parquet igual que lo hicimos para el formato txt
incluyendo la sentencia STORED AS PARQUET. Es importante para comparaciones
posteriores que la tabla padron_parquet conserve los espacios innecesarios y la
tabla padron_parquet_2 no los tenga. Dejo a tu elección cómo hacerlo.

CREATE TABLE padron_parquet_2 (COD_DISTRITO int, DESC_DISTRITO string, COD_DIST_BARRIO int, DESC_BARRIO int, COD_BARRIO string, COD_DIST_SECCION int, COD_SECCION int, COD_EDAD_INT int, EspanolesHombres int, EspanolesMujeres int, ExtranjerosHombres int, ExtranjerosMujeres int) 
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
WITH SERDEPROPERTIES 
(
    "separatorChar" = '\u0059',
    "quoteChar"     = "\""
)   STORED AS PARQUET;


LOAD DATA LOCAL INPATH './Rango_Edades_Seccion_202112.csv' OVERWRITE INTO TABLE padron_parquet_2;

-- Investigar en qué consiste el formato columnar parquet y las ventajas de trabajar con este tipo de formatos.


El formato parquet y otros formatos de columnas manejan una situación común de Hadoop de manera muy eficiente. Es común tener tablas (conjuntos de datos) 
que tengan muchas más columnas de las que cabría esperar en una base de datos relacional bien diseñada; cien o doscientas columnas no son inusuales. 
Esto es así porque a menudo usamos Hadoop como un lugar para desnormalizar datos de formatos relacionales: sí, obtienes muchos valores repetidos y muchas tablas aplanadas en una sola. 
Pero se vuelve mucho más fácil consultar ya que todas las uniones están resueltas. Hay otras ventajas, como la retención de datos del estado en el tiempo. De todos modos, 
es común tener un montón de columnas en una tabla.



Comparar el tamaño de los ficheros de los datos de las tablas padron_txt (txt),
padron_txt_2 (txt pero no incluye los espacios innecesarios), padron_parquet y
padron_parquet_2 (alojados en hdfs cuya ruta se puede obtener de la propiedad
location de cada tabla por ejemplo haciendo "show create table").

DESCRIBE FORMATTED padron_parquet;
hdfs dfs -du -s -h /user/hive/warehouse/datos_padron.db/padron_parquet
-- RESULTADO > 911.6 K  911.6 K 

DESCRIBE FORMATTED padron_parquet_2;
hdfs dfs -du -s -h /user/hive/warehouse/datos_padron.db/padron_parquet_2
-- RESULTADO > 909.6 K  909.6 K


DESCRIBE FORMATTED padron_txt;
hdfs dfs -du -s -h /user/hive/warehouse/datos_padron.db/padron_txt
-- RESULTADO > 21.6 M  21.6 M

DESCRIBE FORMATTED padron_txt_2;
hdfs dfs -du -s -h /user/hive/warehouse/datos_padron.db/padron_txt_2
-- RESULTADO > 11.6 M  11.6 M


-- ¿Qué es Impala?

El proyecto Impala con licencia Apache lleva la tecnología de base de datos escalable en paralelo a Hadoop, permitiendo a los usuarios realizar consultas 
SQL de baja latencia a los datos almacenados en HDFS y Apache HBase sin necesidad de movimiento o transformación de los datos. Impala está integrada con Hadoop 
para utilizar los mismos archivos y formato de datos, metadatos, seguridad y frameworks de gestión de recursos utilizados por MapReduce, Apache Hive, Apache Pig 
y otro software de Hadoop.


-- ¿En qué se diferencia de Hive?

Existen varias diferencias entre Hive e Impala.
Impala tiene un motor distribudo de consultas sobre HDFS , mientras que Hive es un de un sistema de data warehouse sobre hadoop.
Impala tiene una arquitectura MPP ((Massively Parallel Processing)) por ello no usa map/reduce de Hadoop sino que utiliza procesos que se ejecutan en los nodos y 
que consultan directamente sobre HDFS o HBase, por otro lado hive si tiene una arquitectura map/reduce de hadoop.
Impala tiene una latencia menor , mayor consumo de memoria y CPU que hive, el cual es mas robusto en sus consultas.

trabajos pesados de tipo ETL (Extract, Transform and Load) se emplea hive.


-- Comando INVALIDATE METADATA, ¿en qué consiste?

La instrucción INVALIDATE METADATA marca los metadatos de una o todas las tablas como obsoletos. 
La próxima vez que el servicio Impala realiza una consulta en una tabla cuyos metadatos se invalidan,
Impala vuelve a cargar los metadatos asociados antes de que continúe la consulta. 
Como se trata de una operación muy cara en comparación con la actualización incremental de metadatos realizada por la instrucción REFRESH,
cuando sea posible, prefiera REFRESH en lugar de INVALIDATE METADATA.

Hacer invalidate metadata en Impala de la base de datos datos_padron.

INVALIDATE METADATA padron_txt;

Calcular el total de EspanolesHombres, espanolesMujeres, ExtranjerosHombres y
ExtranjerosMujeres agrupado por DESC_DISTRITO y DESC_BARRIO.

SELECT sum(EspanolesHombres), sum(espanolesMujeres), sum(ExtranjerosHombres), sum (extranjerosmujeres)
FROM padron_txt group by DESC_DISTRITO, DESC_BARRIO;

RESULT IMPALA > Fetched 0 row(s) in 7.29s

Llevar a cabo las consultas en Hive en las tablas padron_txt_2 y padron_parquet_2
(No deberían incluir espacios innecesarios). ¿Alguna conclusión?

SELECT sum(EspanolesHombres), sum(espanolesMujeres), sum(ExtranjerosHombres), sum (extranjerosmujeres)
FROM padron_txt_2 group by DESC_DISTRITO, DESC_BARRIO;

RESULT HIVE > Time taken: 39.12 seconds, Fetched: 132 row(s)

SELECT sum(EspanolesHombres), sum(espanolesMujeres), sum(ExtranjerosHombres), sum (extranjerosmujeres)
FROM padron_parquet_2 group by DESC_DISTRITO, DESC_BARRIO;

RESULT HIVE > Time taken: 28.716 seconds, Fetched: 132 row(s)

Llevar a cabo la misma consulta sobre las mismas tablas en Impala. ¿Alguna
conclusión?

SELECT sum(CAST (EspanolesHombres AS INT)), sum( CAST(espanolesMujeres AS INT)), sum(CAST (ExtranjerosHombres AS INT)), sum (CAST (extranjerosmujeres AS INT))
FROM padron_txt_2 group by DESC_DISTRITO, DESC_BARRIO;

RESULT IMPALA > Fetched 132 row(s) in 1.10s

SELECT sum(CAST (EspanolesHombres AS INT)), sum( CAST(espanolesMujeres AS INT)), sum(CAST (ExtranjerosHombres AS INT)), sum (CAST (extranjerosmujeres AS INT))
FROM padron_parquet_2 group by DESC_DISTRITO, DESC_BARRIO;

RESULT IMPALA > Fetched 132 row(s) in 0.57s

Las consultas son unas 30 veces mas rapidas si la tabla esta con formato parquet, hive es mucho mas lento que impala para consultas. 

Crear tabla (Hive) padron_particionado particionada por campos DESC_DISTRITO y
DESC_BARRIO cuyos datos estén en formato parquet.

CREATE TABLE padron_particionado (DESC_DISTRITO string, COD_DIST_BARRIO string, DESC_BARRIO string, COD_DIST_SECCION string, COD_SECCION string, COD_EDAD_INT string, EspanolesHombres string, EspanolesMujeres string, ExtranjerosHombres string, ExtranjerosMujeres string) 
partitioned by (COD_DISTRITO string, COD_BARRIO string)
STORED AS PARQUET;

Insertar datos (en cada partición) dinámicamente (con Hive) en la tabla recién
creada a partir de un select de la tabla padron_parquet_2.

set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;
SET hive.exec.max.dynamic.partitions = 10000;

INSERT INTO TABLE padron_particionado PARTITION(COD_DISTRITO, COD_BARRIO)
SELECT padron_parquet_2.desc_distrito,  padron_parquet_2.cod_dist_barrio,padron_parquet_2.desc_barrio, padron_parquet_2.cod_dist_seccion, padron_parquet_2.cod_seccion,
padron_parquet_2.cod_edad_int, padron_parquet_2.espanoleshombres, padron_parquet_2.espanolesmujeres, padron_parquet_2.extranjeroshombres, padron_parquet_2.extranjerosmujeres,
padron_parquet_2.cod_distrito, padron_parquet_2.cod_barrio FROM padron_parquet_2;

Hacer invalidate metadata en Impala de la base de datos padron_particionado.

invalidate metadata padron_particionado;

Calcular el total de EspanolesHombres, EspanolesMujeres, ExtranjerosHombres y
ExtranjerosMujeres agrupado por DESC_DISTRITO y DESC_BARRIO para los distritos
CENTRO, LATINA, CHAMARTIN, TETUAN, VICALVARO y BARAJAS.

SELECT sum(CAST (p.EspanolesHombres AS INT)), sum( CAST(p.espanolesMujeres AS INT)), sum(CAST (p.ExtranjerosHombres AS INT)), sum (CAST (p.extranjerosmujeres AS INT))
FROM padron_particionado p
WHERE p.COD_BARRIO IN (SELECT COD_BARRIO FROM padron_particionado where DESC_BARRIO IN ("CENTRO", "LATINA", "CHAMARTIN", "TETUAN", "VICALVARO" , "BARAJAS"))
GROUP BY p.DESC_DISTRITO, p.DESC_BARRIO ;

Map: 1  Reduce: 1
HDFS Read: 951042
Time taken: 46.698 seconds

Llevar a cabo la consulta en Hive en las tablas padron_parquet y
padron_partitionado. ¿Alguna conclusión?

SELECT sum(CAST (p.EspanolesHombres AS INT)), sum( CAST(p.espanolesMujeres AS INT)), sum(CAST (p.ExtranjerosHombres AS INT)), sum (CAST (p.extranjerosmujeres AS INT))
FROM padron_parquet p
WHERE p.COD_BARRIO IN (SELECT COD_BARRIO FROM padron_particionado where DESC_BARRIO IN ("CENTRO", "LATINA", "CHAMARTIN", "TETUAN", "VICALVARO" , "BARAJAS"))
GROUP BY p.DESC_DISTRITO, p.DESC_BARRIO ;

Map: 1  Reduce: 1
HDFS Read: 740495
Time taken: 39.184 seconds

La lectura en HDFS es menor para el particionado.

Llevar a cabo la consulta en Impala en las tablas padron_parquet y
padron_particionado. ¿Alguna conclusión?

SELECT sum(CAST (p.EspanolesHombres AS INT)), sum( CAST(p.espanolesMujeres AS INT)), sum(CAST (p.ExtranjerosHombres AS INT)), sum (CAST (p.extranjerosmujeres AS INT))
FROM padron_particionado p
WHERE p.COD_BARRIO IN (SELECT COD_BARRIO FROM padron_particionado where DESC_BARRIO IN ("CENTRO", "LATINA", "CHAMARTIN", "TETUAN", "VICALVARO" , "BARAJAS"))
GROUP BY p.DESC_DISTRITO, p.DESC_BARRIO ;

Fetched 0 row(s) in 7.20s

SELECT sum(CAST (p.EspanolesHombres AS INT)), sum( CAST(p.espanolesMujeres AS INT)), sum(CAST (p.ExtranjerosHombres AS INT)), sum (CAST (p.extranjerosmujeres AS INT))
FROM padron_parquet p
WHERE p.COD_BARRIO IN (SELECT COD_BARRIO FROM padron_particionado where DESC_BARRIO IN ("CENTRO", "LATINA", "CHAMARTIN", "TETUAN", "VICALVARO" , "BARAJAS"))
GROUP BY p.DESC_DISTRITO, p.DESC_BARRIO ;

Fetched 0 row(s) in 2.19s

Hacer consultas de agregación (Max, Min, Avg, Count) tal cual el ejemplo anterior
con las 3 tablas (padron_txt_2, padron_parquet_2 y padron_particionado) y
comparar rendimientos tanto en Hive como en Impala y sacar conclusiones

SELECT Max(CAST (EspanolesHombres AS INT)), Min( CAST(espanolesMujeres AS INT)), Avg(CAST (ExtranjerosHombres AS INT)), Count (CAST (extranjerosmujeres AS INT))
FROM padron_parquet p
WHERE p.COD_BARRIO IN (SELECT COD_BARRIO FROM padron_parquet where DESC_BARRIO IN ("CENTRO", "LATINA", "CHAMARTIN", "TETUAN", "VICALVARO" , "BARAJAS"))
GROUP BY p.DESC_DISTRITO, p.DESC_BARRIO ;

HIVE
HDFS Read: 699121
Time taken: 54.421 seconds

IMPALA
Fetched 0 row(s) in 1.15s

SELECT Max(CAST (EspanolesHombres AS INT)), Min( CAST(espanolesMujeres AS INT)), Avg(CAST (ExtranjerosHombres AS INT)), Count (CAST (extranjerosmujeres AS INT))
FROM padron_parquet_2 p
WHERE p.COD_BARRIO IN (SELECT COD_BARRIO FROM padron_parquet_2 where DESC_BARRIO IN ("CENTRO", "LATINA", "CHAMARTIN", "TETUAN", "VICALVARO" , "BARAJAS"))
GROUP BY p.DESC_DISTRITO, p.DESC_BARRIO ;

HIVE
HDFS Read: 696906
Time taken: 37.776 seconds

IMPALA 
Fetched 0 row(s) in 1.23s

SELECT Max(CAST (EspanolesHombres AS INT)), Min( CAST(espanolesMujeres AS INT)), Avg(CAST (ExtranjerosHombres AS INT)), Count (CAST (extranjerosmujeres AS INT))
FROM padron_particionado p
WHERE p.COD_BARRIO IN (SELECT COD_BARRIO FROM padron_particionado where DESC_BARRIO IN ("CENTRO", "LATINA", "CHAMARTIN", "TETUAN", "VICALVARO" , "BARAJAS"))
GROUP BY p.DESC_DISTRITO, p.DESC_BARRIO ;

HIVE
HDFS Read: 952099
Time taken: 47.023 seconds

IMPALA 
Fetched 0 row(s) in 2.08s


Crear un directorio en HDFS con un nombre a placer, por ejemplo, /test. Si estás en
una máquina Cloudera tienes que asegurarte de que el servicio HDFS está activo ya
que puede no iniciarse al encender la máquina (puedes hacerlo desde el Cloudera
Manager). A su vez, en las máquinas Cloudera es posible (dependiendo de si
usamos Hive desde consola o desde Hue) que no tengamos permisos para crear
directorios en HDFS salvo en el directorio /user/cloudera


hdfs dfs -mkdir /test

Mueve tu fichero datos1 al directorio que has creado en HDFS con un comando
desde consola.

hadoop fs -put -f ./datos1 /test

Desde Hive, crea una nueva database por ejemplo con el nombre numeros. Crea
una tabla que no sea externa y sin argumento location con tres columnas
numéricas, campos separados por coma y delimitada por filas. La llamaremos por
ejemplo numeros_tbl.

CREATE TABLE numeros_tbl ( COL1 int, COL2 int, COL3 int)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ',' 
STORED AS TEXTFILE;

Carga los datos de nuestro fichero de texto datos1 almacenado en HDFS en la tabla
de Hive. Consulta la localización donde estaban anteriormente los datos
almacenados. ¿Siguen estando ahí? ¿Dónde están?. Borra la tabla, ¿qué ocurre con
los datos almacenados en HDFS?

Load data  inpath '/test/datos1' into table numeros_tbl;

el archivo ya no existe en su ruta de hdfs (cortar - pegar), se traslada a nuestra tabla de Hive.
Si realizamos un drop table los datos desaparecen.

Vuelve a mover el fichero de texto datos1 desde el almacenamiento local al
directorio anterior en HDFS.

hadoop fs -put -f ./datos1 /test

Desde Hive, crea una tabla externa sin el argumento location. Y carga datos1 (desde
HDFS) en ella. ¿A dónde han ido los datos en HDFS? Borra la tabla ¿Qué ocurre con
los datos en hdfs?

CREATE EXTERNAL TABLE numeros_tbl2 ( COL1 int, COL2 int, COL3 int)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ',' 
STORED AS TEXTFILE;

Load data  inpath '/test/datos1' into table numeros_tbl2;

ocurre lo mismo que en el caso anterior, los datos son trasladados desde hdfs a hive y 
posteriormente tras borrar la tabla se eliminan estos. 

Borra el fichero datos1 del directorio en el que estén. Vuelve a insertarlos en el
directorio que creamos inicialmente (/test). Vuelve a crear la tabla numeros desde
hive pero ahora de manera externa y con un argumento location que haga
referencia al directorio donde los hayas situado en HDFS (/test). No cargues los
datos de ninguna manera explícita. Haz una consulta sobre la tabla que acabamos
de crear que muestre todos los registros. ¿Tiene algún contenido?


CREATE EXTERNAL TABLE numeros_tbl3 ( COL1 int, COL2 int, COL3 int)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ',' 
STORED AS TEXTFILE
LOCATION '/test';

si , sigue estando los datos en hdfs

Inserta el fichero de datos creado al principio, "datos2" en el mismo directorio de
HDFS que "datos1". Vuelve a hacer la consulta anterior sobre la misma tabla. ¿Qué
salida muestra?

hadoop fs -put -f ./datos2 /test
select * from numeros_tbl3;

la select muestra los datos1 y datos2 que se encuentran dentro de la ruta de hdfs test

Extrae conclusiones de todos estos anteriores apartados.

El comando location resulta mas util si quieres una sincronizacion de una ruta de hdfs con una tabla con hive,
mientras load no mantiene los archivos en hdfs y solo realiza una carga de los datos, una ventaja es no depender en 
si del propio hdfs. 













